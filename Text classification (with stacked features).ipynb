{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are gonna implement basic components in a step by step manner in order to create a text classification framework in python. To start with, import all the required libraries.\n",
    "* Pandas\n",
    "* Scikit-learn\n",
    "* XGBoost\n",
    "* TextBlob\n",
    "* Keras\n",
    "### Libraries for dataset preparation, feature engineering, model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string, scipy, warnings\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from collections.abc import Callable\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dt = open('corpus', encoding='utf-8')\n",
    "data = dt.read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "\n",
    "dt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset. We will implement the following different ideas in order to obtain relevant features from our dataset.\n",
    "\n",
    "2.1 Count Vectors as features<br>\n",
    "2.2 TF-IDF Vectors as features<br>\n",
    "* Word level\n",
    "* N-Gram level\n",
    "* Character level\n",
    "\n",
    "2.3 Word Embeddings as features<br>\n",
    "2.4 Text / NLP based features<br>\n",
    "2.5 Topic Models as features<br>\n",
    "\n",
    "### Implementation of these ideas in detail.\n",
    "\n",
    "2.1 Count Vectors as features<br>\n",
    "Count Vector is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document.<br>\n",
    "Links<br>\n",
    "* https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc\n",
    "* https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_count_and_tfidf = scipy.sparse.hstack((xtrain_count, xtrain_tfidf))\n",
    "xvalid_count_and_tfidf = scipy.sparse.hstack((xvalid_count, xvalid_tfidf))\n",
    "\n",
    "xtrain_count_and_tfidf_ngram = scipy.sparse.hstack((xtrain_count, xtrain_tfidf_ngram))\n",
    "xvalid_count_and_tfidf_ngram = scipy.sparse.hstack((xvalid_count, xvalid_tfidf_ngram))\n",
    "\n",
    "xtrain_count_and_tfidf_ngram_chars = scipy.sparse.hstack((xtrain_count, xtrain_tfidf_ngram_chars))\n",
    "xvalid_count_and_tfidf_ngram_chars = scipy.sparse.hstack((xvalid_count, xvalid_tfidf_ngram_chars))\n",
    "\n",
    "xtrain_tfidf_and_ngram = scipy.sparse.hstack((xtrain_tfidf, xtrain_tfidf_ngram))\n",
    "xvalid_tfidf_and_ngram = scipy.sparse.hstack((xvalid_tfidf, xvalid_tfidf_ngram))\n",
    "\n",
    "xtrain_tfidf_and_ngram_chars = scipy.sparse.hstack((xtrain_tfidf, xtrain_tfidf_ngram_chars))\n",
    "xvalid_tfidf_and_ngram_chars = scipy.sparse.hstack((xvalid_tfidf, xvalid_tfidf_ngram_chars))\n",
    "\n",
    "xtrain_tfidf_ngram_and_ngram_chars = scipy.sparse.hstack((xtrain_tfidf_ngram, xtrain_tfidf_ngram_chars))\n",
    "xvalid_tfidf_ngram_and_ngram_chars = scipy.sparse.hstack((xvalid_tfidf_ngram, xvalid_tfidf_ngram_chars))\n",
    "\n",
    "\n",
    "xtrain_count_and_tfidf_and_ngram = scipy.sparse.hstack((xtrain_count, xtrain_tfidf_and_ngram))\n",
    "xvalid_count_and_tfidf_and_ngram = scipy.sparse.hstack((xvalid_count, xvalid_tfidf_and_ngram))\n",
    "\n",
    "xtrain_count_and_tfidf_and_ngram_chars = scipy.sparse.hstack((xtrain_count, xtrain_tfidf_and_ngram_chars))\n",
    "xvalid_count_and_tfidf_and_ngram_chars = scipy.sparse.hstack((xvalid_count, xvalid_tfidf_and_ngram_chars))\n",
    "\n",
    "xtrain_count_and_tfidf_ngram_and_ngram_chars = scipy.sparse.hstack((xtrain_count, xtrain_tfidf_ngram_and_ngram_chars))\n",
    "xvalid_count_and_tfidf_ngram_and_ngram_chars = scipy.sparse.hstack((xvalid_count, xvalid_tfidf_ngram_and_ngram_chars))\n",
    "\n",
    "xtrain_tfidf_and_tfidf_ngram_and_ngram_chars = scipy.sparse.hstack((xtrain_tfidf, xtrain_tfidf_ngram_and_ngram_chars))\n",
    "xvalid_tfidf_and_tfidf_ngram_and_ngram_chars = scipy.sparse.hstack((xvalid_tfidf, xvalid_tfidf_ngram_and_ngram_chars))\n",
    "\n",
    "xtrain_count_and_tfidf_and_tfidf_ngram_and_ngram_chars = scipy.sparse.hstack((xtrain_count, xtrain_tfidf_and_tfidf_ngram_and_ngram_chars))\n",
    "xvalid_count_and_tfidf_and_tfidf_ngram_and_ngram_chars = scipy.sparse.hstack((xvalid_count, xvalid_tfidf_and_tfidf_ngram_and_ngram_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(7500, 31666)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(7500, 5000)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(7500, 5000)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(7500, 5000)\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "(7500, 36666)\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "(7500, 36666)\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "(7500, 36666)\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "(7500, 10000)\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "(7500, 10000)\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "(7500, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(type(xtrain_count))\n",
    "print(xtrain_count.shape)\n",
    "\n",
    "print(type(xtrain_tfidf))\n",
    "print(xtrain_tfidf.shape)\n",
    "\n",
    "print(type(xtrain_tfidf_ngram))\n",
    "print(xtrain_tfidf_ngram.shape)\n",
    "\n",
    "print(type(xtrain_tfidf_ngram_chars))\n",
    "print(xtrain_tfidf_ngram_chars.shape)\n",
    "\n",
    "print(type(xtrain_count_and_tfidf))\n",
    "print(xtrain_count_and_tfidf.shape)\n",
    "\n",
    "print(type(xtrain_count_and_tfidf_ngram))\n",
    "print(xtrain_count_and_tfidf_ngram.shape)\n",
    "\n",
    "print(type(xtrain_count_and_tfidf_ngram_chars))\n",
    "print(xtrain_count_and_tfidf_ngram_chars.shape)\n",
    "\n",
    "print(type(xtrain_tfidf_and_ngram))\n",
    "print(xtrain_tfidf_and_ngram.shape)\n",
    "\n",
    "print(type(xtrain_tfidf_and_ngram_chars))\n",
    "print(xtrain_tfidf_and_ngram_chars.shape)\n",
    "\n",
    "print(type(xtrain_tfidf_ngram_and_ngram_chars))\n",
    "print(xtrain_tfidf_ngram_and_ngram_chars.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "dt2 = open('wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "for i, line in enumerate(dt2):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "dt2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(7500, 70)\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "(7500, 31736)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_seq_x))\n",
    "print(train_seq_x.shape)\n",
    "\n",
    "xtrain_count_and_word_emb = scipy.sparse.hstack((xtrain_count, train_seq_x))\n",
    "print(type(xtrain_count_and_word_emb))\n",
    "print(xtrain_count_and_word_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = [int(round(p[0])) for p in predictions]\n",
    "        accuracy = metrics.accuracy_score(predictions, valid_y.round())\n",
    "    else:\n",
    "        accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "    \n",
    "    return accuracy, metrics.f1_score(predictions, valid_y, average='weighted', labels=numpy.unique(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors: Accuracy: 0.838 ; F1 score: 0.8382524757930035\n",
      "NB, WordLevel TF-IDF: Accuracy: 0.8452 ; F1 score: 0.8453507710608293\n",
      "NB, N-Gram Vectors: Accuracy: 0.8376 ; F1 score: 0.8376332813312533\n",
      "NB, CharLevel Vectors: Accuracy: 0.812 ; F1 score: 0.8122405385662731\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors and Word Level TF IDF Vectors: Accuracy: 0.84 ; F1 score: 0.8402340111617166\n",
      "NB, Count Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8432 ; F1 score: 0.8434195575551036\n",
      "NB, Count Vectors and Character Level TF IDF Vectors: Accuracy: 0.8388 ; F1 score: 0.8389830843560374\n",
      "NB, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.87 ; F1 score: 0.8700029338369354\n",
      "NB, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8476 ; F1 score: 0.8476850765885363\n",
      "NB, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.852 ; F1 score: 0.8519909053899392\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors and Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_count_and_tfidf, train_y, xvalid_count_and_tfidf)\n",
    "print(\"NB, Count Vectors and Word Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Count Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_count_and_tfidf_ngram, train_y, xvalid_count_and_tfidf_ngram)\n",
    "print(\"NB, Count Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Count Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_count_and_tfidf_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_chars)\n",
    "print(\"NB, Count Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_and_ngram, train_y, xvalid_tfidf_and_ngram)\n",
    "print(\"NB, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_and_ngram_chars, train_y, xvalid_tfidf_and_ngram_chars)\n",
    "print(\"NB, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_and_ngram_chars, train_y, xvalid_tfidf_ngram_and_ngram_chars)\n",
    "print(\"NB, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8464 ; F1 score: 0.8465701869216408\n",
      "NB, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.84 ; F1 score: 0.8401862165151733\n",
      "NB, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8468 ; F1 score: 0.8469572767962685\n",
      "NB, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8468 ; F1 score: 0.8469572767962685\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_count_and_tfidf_and_ngram, train_y, xvalid_count_and_tfidf_and_ngram)\n",
    "print(\"NB, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_count_and_tfidf_and_ngram_chars, train_y, xvalid_count_and_tfidf_and_ngram_chars)\n",
    "print(\"NB, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_count_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"NB, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_count_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"NB, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8488 ; F1 score: 0.8489512197748936\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), xtrain_count_and_tfidf_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"NB, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Word Embeddings: Accuracy: 0.4932 ; F1 score: 0.4936417379611732\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on word-embeddings\n",
    "accuracy, f1_score = train_model(naive_bayes.MultinomialNB(), train_seq_x, train_y, valid_seq_x)\n",
    "print(\"NB, Word Embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors: Accuracy: 0.8696 ; F1 score: 0.8695919869111356\n",
      "LR, WordLevel TF-IDF: Accuracy: 0.8764 ; F1 score: 0.8764205199287448\n",
      "LR, N-Gram Vectors: Accuracy: 0.8364 ; F1 score: 0.8364271606823839\n",
      "LR, CharLevel Vectors: Accuracy: 0.8412 ; F1 score: 0.8412153792835732\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors and Word Level TF IDF Vectors: Accuracy: 0.87 ; F1 score: 0.8699924482841891\n",
      "LR, Count Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.874 ; F1 score: 0.8739912292198038\n",
      "LR, Count Vectors and Character Level TF IDF Vectors: Accuracy: 0.8692 ; F1 score: 0.8691934063876521\n",
      "LR, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8852 ; F1 score: 0.885216264738809\n",
      "LR, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8764 ; F1 score: 0.8764070629713262\n",
      "LR, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8788 ; F1 score: 0.8788069258262518\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors and Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_count_and_tfidf, train_y, xvalid_count_and_tfidf)\n",
    "print(\"LR, Count Vectors and Word Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Count Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_count_and_tfidf_ngram, train_y, xvalid_count_and_tfidf_ngram)\n",
    "print(\"LR, Count Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Count Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_count_and_tfidf_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_chars)\n",
    "print(\"LR, Count Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_tfidf_and_ngram, train_y, xvalid_tfidf_and_ngram)\n",
    "print(\"LR, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_tfidf_and_ngram_chars, train_y, xvalid_tfidf_and_ngram_chars)\n",
    "print(\"LR, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_tfidf_ngram_and_ngram_chars, train_y, xvalid_tfidf_ngram_and_ngram_chars)\n",
    "print(\"LR, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8732 ; F1 score: 0.8731911735323105\n",
      "LR, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8696 ; F1 score: 0.8695939899217503\n",
      "LR, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8752 ; F1 score: 0.8751942480232702\n",
      "LR, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8752 ; F1 score: 0.8751942480232702\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_count_and_tfidf_and_ngram, train_y, xvalid_count_and_tfidf_and_ngram)\n",
    "print(\"LR, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_count_and_tfidf_and_ngram_chars, train_y, xvalid_count_and_tfidf_and_ngram_chars)\n",
    "print(\"LR, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_count_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"LR, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_count_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"LR, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8744 ; F1 score: 0.874393166096001\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), xtrain_count_and_tfidf_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"LR, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, word-embeddings: Accuracy: 0.4888 ; F1 score: 0.4893942240704081\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on word-embeddings\n",
    "accuracy, f1_score = train_model(linear_model.LogisticRegression(solver='lbfgs', max_iter = 4000), train_seq_x, train_y, valid_seq_x)\n",
    "print(\"LR, word-embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors: Accuracy: 0.8488 ; F1 score: 0.8488232377049179\n",
      "SVM, Word Level TF ID: Accuracy: 0.88 ; F1 score: 0.8800129088771582\n",
      "SVM, N-Gram Vectors: Accuracy: 0.88 ; F1 score: 0.8800129088771582\n",
      "SVM, CharLevel Vectors: Accuracy: 0.8512 ; F1 score: 0.8512265860795023\n",
      "SVM, word-embeddings: Accuracy: 0.52 ; F1 score: 0.5203183646005222\n"
     ]
    }
   ],
   "source": [
    "# SVM on Count Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_count, train_y, xvalid_count)\n",
    "print(\"SVM, Count Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"SVM, Word Level TF ID: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "v = train_model(svm.SVC(gamma='scale'), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"SVM, CharLevel Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on word-embeddings\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), train_seq_x, train_y, valid_seq_x)\n",
    "print(\"SVM, word-embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors and Word Level TF IDF Vectors: Accuracy: 0.8504 ; F1 score: 0.8504267290154137\n",
      "SVM, Count Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8508 ; F1 score: 0.8508247700110738\n",
      "SVM, Count Vectors and Character Level TF IDF Vectors: Accuracy: 0.8508 ; F1 score: 0.8508285929416616\n",
      "SVM, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8852 ; F1 score: 0.8852025908036936\n",
      "SVM, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8768 ; F1 score: 0.8768132531138825\n",
      "SVM, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8808 ; F1 score: 0.8808079373860308\n"
     ]
    }
   ],
   "source": [
    "# SVM on Count Vectors and Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_count_and_tfidf, train_y, xvalid_count_and_tfidf)\n",
    "print(\"SVM, Count Vectors and Word Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Count Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_count_and_tfidf_ngram, train_y, xvalid_count_and_tfidf_ngram)\n",
    "print(\"SVM, Count Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Count Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_count_and_tfidf_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_chars)\n",
    "print(\"SVM, Count Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_tfidf_and_ngram, train_y, xvalid_tfidf_and_ngram)\n",
    "print(\"SVM, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_tfidf_and_ngram_chars, train_y, xvalid_tfidf_and_ngram_chars)\n",
    "print(\"SVM, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_tfidf_ngram_and_ngram_chars, train_y, xvalid_tfidf_ngram_and_ngram_chars)\n",
    "print(\"SVM, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.852 ; F1 score: 0.8520227459016394\n",
      "SVM, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8516 ; F1 score: 0.8516284396283016\n",
      "SVM, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.852 ; F1 score: 0.8520227459016394\n",
      "SVM, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.852 ; F1 score: 0.8520227459016394\n"
     ]
    }
   ],
   "source": [
    "# SVM on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_count_and_tfidf_and_ngram, train_y, xvalid_count_and_tfidf_and_ngram)\n",
    "print(\"SVM, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_count_and_tfidf_and_ngram_chars, train_y, xvalid_count_and_tfidf_and_ngram_chars)\n",
    "print(\"SVM, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_count_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"SVM, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_count_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"SVM, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8532 ; F1 score: 0.8532207984639124\n"
     ]
    }
   ],
   "source": [
    "# SVM on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(svm.SVC(gamma='scale'), xtrain_count_and_tfidf_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"SVM, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors: Accuracy: 0.8116 ; F1 score: 0.811670095577896\n",
      "Xgb, WordLevel TF-IDF: Accuracy: 0.8092 ; F1 score: 0.8092709885151941\n",
      "Xgb, Ngram TF-IDF: Accuracy: 0.7436 ; F1 score: 0.7439053147772856\n",
      "Xgb, CharLevel Vectors: Accuracy: 0.8136 ; F1 score: 0.8135885457073287\n",
      "Xgb, word-embeddings: Accuracy: 0.5656 ; F1 score: 0.5672662563948182\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print(\"Xgb, Count Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print(\"Xgb, WordLevel TF-IDF: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "print(\"Xgb, Ngram TF-IDF: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print(\"Xgb, CharLevel Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on word-embeddings\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), train_seq_x, train_y, valid_seq_x)\n",
    "print(\"Xgb, word-embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors and Word Level TF IDF Vectors: Accuracy: 0.8068 ; F1 score: 0.8068592652166948\n",
      "Xgb, Count Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8184 ; F1 score: 0.8185125396922951\n",
      "Xgb, Count Vectors and Character Level TF IDF Vectors: Accuracy: 0.8256 ; F1 score: 0.8256150684010946\n",
      "Xgb, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8124 ; F1 score: 0.8124697979321299\n",
      "Xgb, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.824 ; F1 score: 0.8240408914866368\n",
      "Xgb, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.824 ; F1 score: 0.8239891863108525\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors and Word Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_count_and_tfidf, train_y, xvalid_count_and_tfidf)\n",
    "print(\"Xgb, Count Vectors and Word Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_count_and_tfidf_ngram, train_y, xvalid_count_and_tfidf_ngram)\n",
    "print(\"Xgb, Count Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_count_and_tfidf_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_chars)\n",
    "print(\"Xgb, Count Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_tfidf_and_ngram, train_y, xvalid_tfidf_and_ngram)\n",
    "print(\"Xgb, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_tfidf_and_ngram_chars, train_y, xvalid_tfidf_and_ngram_chars)\n",
    "print(\"Xgb, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_and_ngram_chars, train_y, xvalid_tfidf_ngram_and_ngram_chars)\n",
    "print(\"Xgb, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8132 ; F1 score: 0.8132515619561956\n",
      "Xgb, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.818 ; F1 score: 0.8180215785034829\n",
      "Xgb, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.826 ; F1 score: 0.8260333264049332\n",
      "Xgb, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.826 ; F1 score: 0.8260333264049332\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_count_and_tfidf_and_ngram, train_y, xvalid_count_and_tfidf_and_ngram)\n",
    "print(\"Xgb, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_count_and_tfidf_and_ngram_chars, train_y, xvalid_count_and_tfidf_and_ngram_chars)\n",
    "print(\"Xgb, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_count_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"Xgb, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_count_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"Xgb, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8212 ; F1 score: 0.8212342457540348\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "accuracy, f1_score = train_model(xgboost.XGBClassifier(), xtrain_count_and_tfidf_and_tfidf_ngram_and_ngram_chars, train_y, xvalid_count_and_tfidf_and_tfidf_ngram_and_ngram_chars)\n",
    "print(\"Xgb, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 2s 239us/step - loss: 0.5240\n",
      "NN, Ngram Level TF IDF Vectors: Accuracy: 0.8388 ; F1 score: 0.8389034280405616\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ))\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "# Shallow Neural Network on Ngram Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print(\"NN, Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 10s 1ms/step - loss: 0.4004\n",
      "NN, Count Vectors: Accuracy: 0.8708 ; F1 score: 0.8708125068021316\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 2s 224us/step - loss: 0.4943\n",
      "NN, Word Level TF IDF Vectors: Accuracy: 0.8672 ; F1 score: 0.8676572464676665\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 2s 230us/step - loss: 0.4917\n",
      "NN, Character Level TF IDF Vectors: Accuracy: 0.84 ; F1 score: 0.8400465970399357\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 452.8149\n",
      "NN, word-embeddings: Accuracy: 0.5032 ; F1 score: 0.5042698963463564\n"
     ]
    }
   ],
   "source": [
    "# Shallow Neural Network on Count Vectors\n",
    "classifier = create_model_architecture(xtrain_count.shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_count, train_y, xvalid_count, is_neural_net=True)\n",
    "print(\"NN, Count Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Word Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_tfidf.shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_tfidf, train_y, xvalid_tfidf, is_neural_net=True)\n",
    "print(\"NN, Word Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Character Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram_chars.shape[1])\n",
    "accuracy, f1_score = train_model(classifier,  xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, is_neural_net=True)\n",
    "print(\"NN, Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on word-embeddings\n",
    "classifier = create_model_architecture(train_seq_x.shape[1])\n",
    "accuracy, f1_score = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"NN, word-embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(xtrain_count))\n",
    "print(type(xtrain_count_and_tfidf.tocsr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 20s 3ms/step - loss: 0.3978\n",
      "NN, Count Vectors and Word Level TF IDF Vectors: Accuracy: 0.8712 ; F1 score: 0.8712457563489141\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 20s 3ms/step - loss: 0.3878\n",
      "NN, Count Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8748 ; F1 score: 0.8748049078596316\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 21s 3ms/step - loss: 0.3897: 2s - ETA: 1s -\n",
      "NN, Count Vectors and Character Level TF IDF Vectors: Accuracy: 0.874 ; F1 score: 0.8741098243549879\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 7s 961us/step - loss: 0.4337\n",
      "NN, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8892 ; F1 score: 0.8892025005317878\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 7s 975us/step - loss: 0.4387\n",
      "NN, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.878 ; F1 score: 0.8779907270727072\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 7s 969us/step - loss: 0.4378\n",
      "NN, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8764 ; F1 score: 0.8764379411938494\n"
     ]
    }
   ],
   "source": [
    "# Shallow Neural Network on Count Vectors and Word Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_count_and_tfidf.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_count_and_tfidf.tocsr(), train_y, xvalid_count_and_tfidf.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Count Vectors and Word Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Count Vectors and Ngram Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_count_and_tfidf_ngram.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_count_and_tfidf_ngram.tocsr(), train_y, xvalid_count_and_tfidf_ngram.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Count Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Count Vectors and Character Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_count_and_tfidf_ngram_chars.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_count_and_tfidf_ngram_chars.tocsr(), train_y, xvalid_count_and_tfidf_ngram_chars.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Count Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_tfidf_and_ngram.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_tfidf_and_ngram.tocsr(), train_y, xvalid_tfidf_and_ngram.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_tfidf_and_ngram_chars.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_tfidf_and_ngram_chars.tocsr(), train_y, xvalid_tfidf_and_ngram_chars.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram_and_ngram_chars.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_tfidf_ngram_and_ngram_chars.tocsr(), train_y, xvalid_tfidf_ngram_and_ngram_chars.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 23s 3ms/step - loss: 0.3836\n",
      "NN, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy: 0.8772 ; F1 score: 0.8772027703719893\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 23s 3ms/step - loss: 0.3774: 0s - loss: 0.377\n",
      "NN, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8688 ; F1 score: 0.8689453679955681\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 23s 3ms/step - loss: 0.3790\n",
      "NN, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8776 ; F1 score: 0.877749451699539\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 23s 3ms/step - loss: 0.3769\n",
      "NN, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.8828 ; F1 score: 0.8827963424699036\n"
     ]
    }
   ],
   "source": [
    "# Shallow Neural Network on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_count_and_tfidf_and_ngram.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_count_and_tfidf_and_ngram.tocsr(), train_y, xvalid_count_and_tfidf_and_ngram.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_count_and_tfidf_and_ngram_chars.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_count_and_tfidf_and_ngram_chars.tocsr(), train_y, xvalid_count_and_tfidf_and_ngram_chars.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Count Vectors and Word Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_count_and_tfidf_ngram_and_ngram_chars.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_count_and_tfidf_ngram_and_ngram_chars.tocsr(), train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Count Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)\n",
    "\n",
    "# Shallow Neural Network on Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_count_and_tfidf_ngram_and_ngram_chars.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_count_and_tfidf_ngram_and_ngram_chars.tocsr(), train_y, xvalid_count_and_tfidf_ngram_and_ngram_chars.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 25s 3ms/step - loss: 0.3668\n",
      "NN, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy: 0.878 ; F1 score: 0.8781981353383459\n"
     ]
    }
   ],
   "source": [
    "# Shallow Neural Network on Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors\n",
    "classifier = create_model_architecture(xtrain_count_and_tfidf_and_tfidf_ngram_and_ngram_chars.tocsr().shape[1])\n",
    "accuracy, f1_score = train_model(classifier, xtrain_count_and_tfidf_and_tfidf_ngram_and_ngram_chars.tocsr(), train_y, xvalid_count_and_tfidf_and_tfidf_ngram_and_ngram_chars.tocsr(), is_neural_net=True)\n",
    "print(\"NN, Count Vectors and Word Level TF IDF Vectors and Ngram Level TF IDF Vectors and Character Level TF IDF Vectors: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 27s 4ms/step - loss: 0.5588\n",
      "CNN, Word Embeddings: Accuracy: 0.8228 ; F1 score: 0.8228339589092657\n"
     ]
    }
   ],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy, f1_score = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 70s 9ms/step - loss: 0.5946\n",
      "RNN-LSTM, Word Embeddings: Accuracy: 0.7976 ; F1 score: 0.7976062200209485\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy, f1_score = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 70s 9ms/step - loss: 0.6103\n",
      "RNN-GRU, Word Embeddings: Accuracy: 0.7652 ; F1 score: 0.7724660856614499\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_gru():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy, f1_score = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-GRU, Word Embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 104s 14ms/step - loss: 0.6161\n",
      "RNN-Bidirectional, Word Embeddings: Accuracy: 0.792 ; F1 score: 0.79248\n"
     ]
    }
   ],
   "source": [
    "def create_bidirectional_rnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "accuracy, f1_score = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-Bidirectional, Word Embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 35s 5ms/step - loss: 0.5780\n",
      "RCNN, Word Embeddings: Accuracy: 0.8276 ; F1 score: 0.8276584451606819\n"
     ]
    }
   ],
   "source": [
    "def create_rcnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy, f1_score = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RCNN, Word Embeddings: Accuracy:\", accuracy, \"; F1 score:\", f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
